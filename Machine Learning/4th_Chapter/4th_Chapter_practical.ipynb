{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071f6a6e",
   "metadata": {},
   "source": [
    "# Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e12cc7",
   "metadata": {},
   "source": [
    "This is the function that we are trying to minimize.it measures how well ur model fits your training data.We want the output of this function to be as low as possible.\n",
    "\n",
    "An Example of cost function would be MSE or mean Squared Error which is essentially the mean of the residuals^2(diffrence between actual points and predicted points) of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913eda0",
   "metadata": {},
   "source": [
    "# Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb84802",
   "metadata": {},
   "source": [
    "How do we calculate the minimum value to our cost function?\n",
    "\n",
    "we Use something called an Optimization Algorithm which can be either help us calculate the optimal solution or something close to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbf7de",
   "metadata": {},
   "source": [
    "# Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de5f9c",
   "metadata": {},
   "source": [
    "The Normal equation is the eqation which directly gives us the result to the minimum of our cost function\n",
    "\n",
    "\n",
    "The Normal Equation works by representing the cost equation as matrix and finding what's called the Moore-Penrose inverse or the psudouniverse of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442682a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(1000,1) # creates a dataset\n",
    "\n",
    "y = 4 + 3 * X + np.random.randn(1000,1) # creates a dataset\n",
    "\n",
    "X_b = np.c_[np.ones((1000,1)),X] # add x0 = 1 to each intance\n",
    "\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49f8c5",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f1beb",
   "metadata": {},
   "source": [
    "This is a generic optimization algorithm that can be used to find optimal solution to bunch of problems.It can be less computational intensive than directly calculating the normal equation.\n",
    "Gradient Descent is affected by the scale of X values so you should make sure everything is on the same scale.We would want to use Gradient Descent for problems where we can't calculate the normal equation because of computational complexity\n",
    "\n",
    "Gradient Descent woks by randomly placing a point somewhere in the cost function and moving the point downwards until it reaches thr minimum point.\n",
    "The Size of the \"Steps\" taken to get to the minimum is called the Learning rate and randomly picking a point to start on the cost function graph to start is called Random Initialization.\n",
    "\n",
    "Random Initialization : Filling theta with random values in initilizing a gradient Descent equation.\n",
    "\n",
    "Learning Rate : The Size of the steps the gradient descent algorithm will take.\n",
    "small steps will lead to a more exact result but a will take to compute larger steps could lead to a less accurate result but will get you there much faster.\n",
    "\n",
    "Local Minima: A local Minimum is minimu that isn't the absilute minimum for the cost function.A problem with gradient descent is that you might confuse a local minimum with a global maximum.\n",
    "\n",
    "Global Minima/Maxima : The absolute minimum of the cost function\n",
    "\n",
    "Gradient Descent can be slow depending on whether you have a lot of training data or features to train on. In real life time will be a major constraint as u ll'be expected to output a deliverable in a good amount of time,the below methods of gradient Descent can help speed up the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343a849",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97879bc0",
   "metadata": {},
   "source": [
    "calucaltes the gradient descent using the entire training dataset at each step.Because of this Btach gradient can be very slow when you have a lot of training data but scales well with the number of features you have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f076088",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52172fa",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent picks a random instance from your dataset and calucaltes the gradient on that instance switching the intance each time until you reach the minima. it will typically reach a good minima but not the optimal one.This is great if u have a massive training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d80df",
   "metadata": {},
   "source": [
    "# Mini- Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821b169",
   "metadata": {},
   "source": [
    "same as Batch gradient Descent but picks a random batch of instances to use to calculate the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80070f",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee334ab",
   "metadata": {},
   "source": [
    "When using cross validation (splitting your training dataset into sections to train multiple times), the learning curve plots the value of the cost function vs the number of training instances for both the training set and validation set.This can be used to determine whetther your model suffers from too much bias or varinace. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6cca9",
   "metadata": {},
   "source": [
    "BiAS : Genralization erroe due to wronfg assumptions.Usually leads to underfitting.\n",
    "\n",
    "VARIANCE : Genralization error that is due to execessive sensitivity to small variations in the training data.This leads to overfitting.\n",
    "\n",
    "Irreducible Error : Noiseness inherent to the data. The only way to get rid of this to clean the data.\n",
    "\n",
    "BIAS/VARIANCE TRADEOFF : inccreasing compleixty usually increase variance at the cost of bias and reducing complexity does the opposite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a788d",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc9987",
   "metadata": {},
   "source": [
    "Sometimes when we train a model it will start to overfit. A way to avoid overfitting data (especially for models like linear regressions that are hevily affected by outliers) we can use regularizations.this will lead to more genral model in that is technically less accurate but genralize to the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f1d53",
   "metadata": {},
   "source": [
    "# Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e476d78",
   "metadata": {},
   "source": [
    "Early stopping is a simple way to regularize iterative learning algorithms like gradient Descent by stopping training when the validation erroe reaches a minimum.You can reduce the chances of finding a local minima by enforcing that the validation set eroor needs to increase for a certain amount of time before stopping training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b69a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
